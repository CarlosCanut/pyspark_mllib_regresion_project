{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de Tenis para elegir pista "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción del conjunto de datos y tarea a realizar\n",
    "Para este proyecto vamos a utilizar el conjunto de datos de los partidos realizados en la ATP desde el año 2010 hasta el 2021, con esto realizaremos una serie de modelos de clasificación con los que predecir en base a las caracteristicas de un jugador vencedor y de otro perdedor, además del tiempo de duración del partido y el ranking de la ATP, cual sería el tipo de superficie más idonea para que el resultado planteado ocurriera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>surface</th><th>tourney_level</th><th>tourney_date</th><th>winner_hand</th><th>winner_ht</th><th>winner_age</th><th>loser_hand</th><th>loser_ht</th><th>loser_age</th><th>score</th><th>minutes</th><th>w_ace</th><th>w_df</th><th>w_svpt</th><th>w_1stIn</th><th>w_1stWon</th><th>w_2ndWon</th><th>w_SvGms</th><th>w_bpSaved</th><th>w_bpFaced</th><th>l_ace</th><th>l_df</th><th>l_svpt</th><th>l_1stIn</th><th>l_1stWon</th><th>l_2ndWon</th><th>l_SvGms</th><th>l_bpSaved</th><th>l_bpFaced</th><th>winner_rank</th><th>winner_rank_points</th><th>loser_rank</th><th>loser_rank_points</th><th>year</th></tr>\n",
       "<tr><td>Hard</td><td>A</td><td>20100103</td><td>R</td><td>188.0</td><td>27.3483915127</td><td>R</td><td>183.0</td><td>30.3463381246</td><td>7-6(5) 6-2</td><td>84.0</td><td>15.0</td><td>0.0</td><td>63.0</td><td>42.0</td><td>36.0</td><td>14.0</td><td>10.0</td><td>3.0</td><td>3.0</td><td>4.0</td><td>2.0</td><td>56.0</td><td>34.0</td><td>29.0</td><td>11.0</td><td>10.0</td><td>3.0</td><td>5.0</td><td>7.0</td><td>4410.0</td><td>77.0</td><td>598.0</td><td>2010</td></tr>\n",
       "<tr><td>Hard</td><td>A</td><td>20100103</td><td>L</td><td>198.0</td><td>22.5434633812</td><td>L</td><td>190.0</td><td>22.3709787817</td><td>7-5 6-1</td><td>70.0</td><td>10.0</td><td>3.0</td><td>57.0</td><td>30.0</td><td>23.0</td><td>19.0</td><td>10.0</td><td>0.0</td><td>0.0</td><td>2.0</td><td>2.0</td><td>66.0</td><td>34.0</td><td>22.0</td><td>14.0</td><td>9.0</td><td>7.0</td><td>10.0</td><td>134.0</td><td>400.0</td><td>78.0</td><td>590.0</td><td>2010</td></tr>\n",
       "<tr><td>Hard</td><td>A</td><td>20100103</td><td>R</td><td>185.0</td><td>23.5482546201</td><td>L</td><td>185.0</td><td>28.4517453799</td><td>6-3 4-6 6-4</td><td>121.0</td><td>5.0</td><td>4.0</td><td>97.0</td><td>51.0</td><td>33.0</td><td>27.0</td><td>15.0</td><td>5.0</td><td>8.0</td><td>4.0</td><td>0.0</td><td>85.0</td><td>58.0</td><td>38.0</td><td>14.0</td><td>14.0</td><td>7.0</td><td>11.0</td><td>52.0</td><td>850.0</td><td>88.0</td><td>568.0</td><td>2010</td></tr>\n",
       "<tr><td>Hard</td><td>A</td><td>20100103</td><td>R</td><td>188.0</td><td>22.1081451061</td><td>L</td><td>183.0</td><td>28.6214921287</td><td>7-5 6-1</td><td>64.0</td><td>12.0</td><td>1.0</td><td>50.0</td><td>35.0</td><td>30.0</td><td>12.0</td><td>10.0</td><td>3.0</td><td>3.0</td><td>2.0</td><td>1.0</td><td>53.0</td><td>29.0</td><td>16.0</td><td>15.0</td><td>9.0</td><td>2.0</td><td>5.0</td><td>285.0</td><td>151.0</td><td>28.0</td><td>1260.0</td><td>2010</td></tr>\n",
       "<tr><td>Hard</td><td>A</td><td>20100103</td><td>R</td><td>196.0</td><td>24.2984257358</td><td>R</td><td>183.0</td><td>21.4291581109</td><td>6-2 6-4</td><td>69.0</td><td>3.0</td><td>1.0</td><td>46.0</td><td>27.0</td><td>24.0</td><td>14.0</td><td>9.0</td><td>1.0</td><td>1.0</td><td>6.0</td><td>1.0</td><td>69.0</td><td>41.0</td><td>26.0</td><td>14.0</td><td>9.0</td><td>6.0</td><td>9.0</td><td>20.0</td><td>1655.0</td><td>251.0</td><td>179.0</td><td>2010</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+-------+-------------+------------+-----------+---------+-------------+----------+--------+-------------+-----------+-------+-----+----+------+-------+--------+--------+-------+---------+---------+-----+----+------+-------+--------+--------+-------+---------+---------+-----------+------------------+----------+-----------------+----+\n",
       "|surface|tourney_level|tourney_date|winner_hand|winner_ht|   winner_age|loser_hand|loser_ht|    loser_age|      score|minutes|w_ace|w_df|w_svpt|w_1stIn|w_1stWon|w_2ndWon|w_SvGms|w_bpSaved|w_bpFaced|l_ace|l_df|l_svpt|l_1stIn|l_1stWon|l_2ndWon|l_SvGms|l_bpSaved|l_bpFaced|winner_rank|winner_rank_points|loser_rank|loser_rank_points|year|\n",
       "+-------+-------------+------------+-----------+---------+-------------+----------+--------+-------------+-----------+-------+-----+----+------+-------+--------+--------+-------+---------+---------+-----+----+------+-------+--------+--------+-------+---------+---------+-----------+------------------+----------+-----------------+----+\n",
       "|   Hard|            A|    20100103|          R|    188.0|27.3483915127|         R|   183.0|30.3463381246| 7-6(5) 6-2|   84.0| 15.0| 0.0|  63.0|   42.0|    36.0|    14.0|   10.0|      3.0|      3.0|  4.0| 2.0|  56.0|   34.0|    29.0|    11.0|   10.0|      3.0|      5.0|        7.0|            4410.0|      77.0|            598.0|2010|\n",
       "|   Hard|            A|    20100103|          L|    198.0|22.5434633812|         L|   190.0|22.3709787817|    7-5 6-1|   70.0| 10.0| 3.0|  57.0|   30.0|    23.0|    19.0|   10.0|      0.0|      0.0|  2.0| 2.0|  66.0|   34.0|    22.0|    14.0|    9.0|      7.0|     10.0|      134.0|             400.0|      78.0|            590.0|2010|\n",
       "|   Hard|            A|    20100103|          R|    185.0|23.5482546201|         L|   185.0|28.4517453799|6-3 4-6 6-4|  121.0|  5.0| 4.0|  97.0|   51.0|    33.0|    27.0|   15.0|      5.0|      8.0|  4.0| 0.0|  85.0|   58.0|    38.0|    14.0|   14.0|      7.0|     11.0|       52.0|             850.0|      88.0|            568.0|2010|\n",
       "|   Hard|            A|    20100103|          R|    188.0|22.1081451061|         L|   183.0|28.6214921287|    7-5 6-1|   64.0| 12.0| 1.0|  50.0|   35.0|    30.0|    12.0|   10.0|      3.0|      3.0|  2.0| 1.0|  53.0|   29.0|    16.0|    15.0|    9.0|      2.0|      5.0|      285.0|             151.0|      28.0|           1260.0|2010|\n",
       "|   Hard|            A|    20100103|          R|    196.0|24.2984257358|         R|   183.0|21.4291581109|    6-2 6-4|   69.0|  3.0| 1.0|  46.0|   27.0|    24.0|    14.0|    9.0|      1.0|      1.0|  6.0| 1.0|  69.0|   41.0|    26.0|    14.0|    9.0|      6.0|      9.0|       20.0|            1655.0|     251.0|            179.0|2010|\n",
       "+-------+-------------+------------+-----------+---------+-------------+----------+--------+-------------+-----------+-------+-----+----+------+-------+--------+--------+-------+---------+---------+-----+----+------+-------+--------+--------+-------+---------+---------+-----------+------------------+----------+-----------------+----+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "\n",
    "### Cargamos el conjunto de datos con todos los partidos de la ATP desde 2010 hasta 2021\n",
    "all_atp_games = spark.read.csv(\"file:///home/alumno/Desktop/pyspark_mllib_regresion_project/datasets/all_atp_games.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "\n",
    "### Ahora realizamos una limpieza de las columnas que no vamos a utilizar\n",
    "# primero eliminamos las columnas que nos dan información sobre el torneo o datos externos al juego en si\n",
    "columns_to_drop = ['tourney_name', 'tourney_id', 'draw_size', 'match_num', 'winner_id', 'loser_id', 'winner_name', 'loser_name',\n",
    "                  'winner_seed', 'winner_entry', 'winner_ioc', 'loser_seed', 'loser_entry', 'loser_ioc',\n",
    "                  'best_of', 'round']\n",
    "all_atp_games = all_atp_games.drop(*columns_to_drop)\n",
    "\n",
    "# vamos a eliminar todos los registros los cuales no cuentes con la altura del vencedor o perdedor\n",
    "all_atp_games = all_atp_games.dropna('any', subset=['winner_ht', 'loser_ht'])\n",
    "# tambien los datos sin el terreno definido los eliminaremos\n",
    "all_atp_games = all_atp_games.dropna('all', subset=['surface'])\n",
    "\n",
    "# creamos una nueva columna con el año que ocurrió el partido\n",
    "all_atp_games = all_atp_games.withColumn(\"year\", expr('LEFT(tourney_date,4)'))\n",
    "\n",
    "\n",
    "all_atp_games.limit(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones con API de datos estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para las variables de respuesta (categorica), calcular estadísticas agregadas de las variables predictoras\n",
    "# en base a la de respuesta\n",
    "\n",
    "# Vamos a observar la media, minimo y máximo de edad y altura del ganador y perdedor en cada tipo de superficie\n",
    "height_by_surface = all_atp_games.groupBy(col('surface')).agg(expr('mean(winner_ht)'),expr('mean(loser_ht)'),\n",
    "                                         expr('max(winner_ht)'),expr('max(loser_ht)'),\n",
    "                                         expr('min(winner_ht)'),expr('min(loser_ht)'))\n",
    "\n",
    "age_by_surface = all_atp_games.groupBy(col('surface')).agg(expr('mean(winner_age)'),expr('mean(loser_age)'),\n",
    "                                         expr('max(winner_age)'),expr('max(loser_age)'),\n",
    "                                         expr('min(winner_age)'),expr('min(loser_age)'))\n",
    "\n",
    "rank_by_surface = all_atp_games.groupBy(col('surface')).agg(expr('mean(winner_rank)'),expr('mean(loser_rank)'),\n",
    "                                         expr('max(winner_rank)'),expr('max(loser_rank)'),\n",
    "                                         expr('min(winner_rank)'),expr('min(loser_rank)'))\n",
    "\n",
    "\n",
    "height_by_surface\n",
    "age_by_surface.show()\n",
    "rank_by_surface.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar independientemente del tipo de pista, los ganadores son de media más altos, más jovenes y con una mejor posición en el ranking.\n",
    "A excepción de los jugadores en Grass (Hierba) donde la media de los jugadores ganadores son mayores, esto es interesante aunque habría que analizar con más profundidad para deducir algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a limpiar todos los valores null ya que no queremos ni valores nulos y 0 introducidos en nulos para el entrenamiento\n",
    "all_atp_games = all_atp_games.dropna('any')\n",
    "# Contamos con demasiados registros, así que vamos a reducir esto, primero vamos a ver cuantos registros tenemos de cada año\n",
    "print(\"Datos totales:\", all_atp_games.count())\n",
    "all_atp_games.groupBy(\"year\").count().show()\n",
    "# contamos con 12 años diferentes, vamos a tratar de hacer un muestreo estratificado para obtener\n",
    "# un conjunto reducido.\n",
    "porcentaje_por_clase = 0.125\n",
    "seed_stratified = 12\n",
    "fracciones = all_atp_games.select(\"year\").distinct().withColumn(\"fraccion\", lit(porcentaje_por_clase)).rdd.collectAsMap()\n",
    "all_atp_games_reducido = all_atp_games.stat.sampleBy(\"year\", fracciones, seed_stratified)\n",
    "\n",
    "# vamos a comprobar la distribución y total de muestras actuales\n",
    "print(\"Nuevos datos reducidos:\", all_atp_games_reducido.count())\n",
    "all_atp_games_reducido.groupBy(\"year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a proyectar con las variables de winner_rank, winner_ht, loser_rank, loser_ht y minutes. \n",
    "# Ademas de surface (variable a predecir).\n",
    "projected_games = all_atp_games_reducido.select('surface','winner_ht','loser_ht','minutes','winner_rank','loser_rank')\n",
    "projected_games.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis variable a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# altura\n",
    "height_resume_winner = all_atp_games.selectExpr('mean(winner_ht) as mean_winner', 'percentile(winner_ht, 0.5) as median_winner',\n",
    "                                         'percentile(winner_ht, 0.25) as Q1_winner', 'percentile(winner_ht, 0.75) as Q3_winner',\n",
    "                                        'kurtosis(winner_ht) as kurtosis_winner', 'skewness(winner_ht) as skewness_winner')\n",
    "\n",
    "height_resume_loser = all_atp_games.selectExpr('mean(loser_ht) as mean_loser', 'percentile(loser_ht, 0.5) as median_loser',\n",
    "                                         'percentile(loser_ht, 0.25) as Q1_loser', 'percentile(loser_ht, 0.75) as Q3_loser',\n",
    "                                        'kurtosis(loser_ht) as kurtosis_loser', 'skewness(loser_ht) as skewness_loser')\n",
    "# edad\n",
    "age_resume_winner = all_atp_games.selectExpr('mean(winner_age) as mean_winner', 'percentile(winner_age, 0.5) as median_winner',\n",
    "                                         'percentile(winner_age, 0.25) as Q1_winner', 'percentile(winner_age, 0.75) as Q3_winner',\n",
    "                                        'kurtosis(winner_age) as kurtosis_winner', 'skewness(winner_age) as skewness_winner')\n",
    "\n",
    "age_resume_loser = all_atp_games.selectExpr('mean(loser_age) as mean_loser', 'percentile(loser_age, 0.5) as median_loser',\n",
    "                                         'percentile(loser_age, 0.25) as Q1_loser', 'percentile(loser_age, 0.75) as Q3_loser',\n",
    "                                        'kurtosis(loser_age) as kurtosis_loser', 'skewness(loser_age) as skewness_loser')\n",
    "\n",
    "# rank\n",
    "rank_resume_winner = all_atp_games.selectExpr('mean(winner_rank) as mean_winner', 'percentile(winner_rank, 0.5) as median_winner',\n",
    "                                         'percentile(winner_rank, 0.25) as Q1_winner', 'percentile(winner_rank, 0.75) as Q3_winner',\n",
    "                                        'kurtosis(winner_rank) as kurtosis_winner', 'skewness(winner_rank) as skewness_winner')\n",
    "\n",
    "rank_resume_loser = all_atp_games.selectExpr('mean(loser_rank) as mean_loser', 'percentile(loser_rank, 0.5) as median_loser',\n",
    "                                         'percentile(loser_rank, 0.25) as Q1_loser', 'percentile(loser_rank, 0.75) as Q3_loser',\n",
    "                                        'kurtosis(loser_rank) as kurtosis_loser', 'skewness(loser_rank) as skewness_loser')\n",
    "\n",
    "# minute\n",
    "minute_resume = all_atp_games.selectExpr('mean(minutes) as mean', 'percentile(minutes, 0.5) as median',\n",
    "                                         'percentile(minutes, 0.25) as Q1', 'percentile(minutes, 0.75) as Q3',\n",
    "                                        'kurtosis(minutes) as kurtosis', 'skewness(minutes) as skewness')\n",
    "minute_resume_by_surface = all_atp_games.groupBy('surface').agg(expr('mean(minutes) as mean'), expr('percentile(minutes, 0.5) as median'),\n",
    "                                                               expr('percentile(minutes, 0.25) as Q1'),\n",
    "                                                               expr('percentile(minutes, 0.75) as Q3'),\n",
    "                                                               expr('kurtosis(minutes) as kurtosis'),\n",
    "                                                               expr('skewness(minutes) as skewness'),\n",
    "                                                               expr('count(minutes) as count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a transformar el conjunto de datos a un df de pandas para observar el histograma de cada variable\n",
    "all_atp_games_df = all_atp_games.toPandas()\n",
    "\n",
    "print(\"resumen height\")\n",
    "height_resume_winner.show()\n",
    "all_atp_games_df.hist(column = \"winner_ht\")\n",
    "height_resume_loser.show()\n",
    "all_atp_games_df.hist(column = \"loser_ht\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La altura de los jugadores parece seguir una distribución aproximadamente normal observando su histograma, kurtosis y asimetría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resumen age\")\n",
    "age_resume_winner.show()\n",
    "all_atp_games_df.hist(column = \"winner_age\")\n",
    "age_resume_loser.show()\n",
    "all_atp_games_df.hist(column = \"loser_age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La edad de los jugadores parece seguir una distribución aproximadamente normal observando su histograma, kurtosis y asimetría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resumen rank\")\n",
    "rank_resume_winner.show()\n",
    "all_atp_games_df.hist(column = \"winner_rank\")\n",
    "rank_resume_loser.show()\n",
    "all_atp_games_df.hist(column = \"loser_rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El ranking de los jugadores no es normal, pero esto es normal ya que estamos trabajando con todas las partidas, y dado que el rango que necesitan los jugadores para participar en estos tipos de torneos es alto, obviamente en la mayor parte de los partidos están todos agrupados en los primeros 200 del ATP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resumen game lenght\")\n",
    "minute_resume.show()\n",
    "all_atp_games_df.hist(column = \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la duración de las partidas, podemos observar que suelen durar una hora y 40 minutos de media, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resumen game lenght by surface\")\n",
    "minute_resume_by_surface.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar la duración de las partidas por el tipo de pista, podemos ver que la pista de alfombra tiene una duración más alta de media, esto podemos ver que se debe a que únicamente tenemos 1 registro de este tipo de pista.\n",
    "Por otro lado se observa que en cesped la media es ligeramente superior, lo cual puede deberse a que cesped es el tipo de cancha más rápido, lo que pueda ayudar a que los jugadores puedan aguantar más en cada punto y esto haga que dure más la partida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_resume_winner_surface = all_atp_games.groupBy('winner_hand').agg(count('surface').alias('count'), (count('surface')/all_atp_games.count()).alias('percentage'))\n",
    "hand_resume_loser_surface = all_atp_games.groupBy('loser_hand').agg(count('surface').alias('count'), (count('surface')/all_atp_games.count()).alias('percentage'))\n",
    "\n",
    "hand_resume_winner_surface.show()\n",
    "hand_resume_loser_surface.show()\n",
    "\n",
    "\n",
    "all_atp_games.crosstab('tourney_level','winner_hand').orderBy('tourney_level_winner_hand', ascending=True).show(n=50)\n",
    "\n",
    "all_atp_games.filter(\"winner_rank < 50\").crosstab('winner_rank','winner_hand').orderBy('winner_rank_winner_hand', ascending=True).show(n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que la dominancia de la mano derecha es constante en todos los niveles de juego y todos los jugadores que han tenido cierto rango en el ranking ATP, lo que es entendible ya que generalmente la gente suele ser diestra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento y evaluación de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tarea de clasificación, teniendo en cuenta las siguientes variables de los jugadores (a la hora de introducir los datos habrá que predefinir dos jugadores, uno como ganador y otro como perdedor):\n",
    "- hand: mano dominante\n",
    "- tourney_level: Nivel de la competición donde se juega el partido\n",
    "- ht: altura\n",
    "- age: edad\n",
    "- rank: posición en el ranking ATP\n",
    "- minutes: duración en minutos de la partida\n",
    "\n",
    "Trataremos de predecir que tipo de superficie sería la más idonea para que el jugador introducido como vencedor obtenga la victoria en la duración de partida introducida:\n",
    "- surface: tipo de pista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft \n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# preparamos el dataset con las variables que vamos a utilizar\n",
    "games_df = all_atp_games_reducido.select('winner_hand','loser_hand','winner_ht','loser_ht','winner_age','loser_age','winner_rank','loser_rank','minutes','tourney_level','surface')\n",
    "games_df = games_df.withColumnRenamed(\"winner_hand\",\"w_hand\")\n",
    "games_df = games_df.withColumnRenamed(\"loser_hand\",\"l_hand\")\n",
    "games_df = games_df.withColumnRenamed(\"winner_ht\",\"w_height\")\n",
    "games_df = games_df.withColumnRenamed(\"loser_ht\",\"l_height\")\n",
    "games_df = games_df.withColumnRenamed(\"winner_age\",\"w_age\")\n",
    "games_df = games_df.withColumnRenamed(\"loser_age\",\"l_age\")\n",
    "games_df = games_df.withColumnRenamed(\"winner_rank\",\"w_rank\")\n",
    "games_df = games_df.withColumnRenamed(\"loser_rank\",\"l_rank\")\n",
    "games_df.show(5)\n",
    "games_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero total de clases a predecir\n",
    "clases = games_df.groupBy('surface').agg(expr('count(minutes) as count'))\n",
    "clases.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora cargamos un conjunto de entrenamiento y otro de test\n",
    "train_df, test_df = games_df.randomSplit(weights = [0.70, 0.30], seed = 2021)\n",
    "print(\"conjunto de entrenamiento: \", train_df.count(), \", conjunto de test: \", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESTIMADORES Y TRANSFORMADORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "### ESTIMADORES Y TRANSFORMADORES\n",
    "# binarizer\n",
    "binarizer_rank = ft.Binarizer(threshold=50,inputCols=[\"w_rank\",\"l_rank\"], outputCols=[\"w_rank_bin\",\"l_rank_bin\"])\n",
    "binarizer_min = ft.Binarizer(threshold=108,inputCol=\"minutes\", outputCol=\"minutes_bin\")\n",
    "# string indexer\n",
    "indexer = ft.StringIndexer(inputCols=['w_hand','l_hand','tourney_level','surface'],outputCols=['w_hand_class','l_hand_class','tourney_level_class','surface_class'])\n",
    "# one hot encoder\n",
    "onehotencoder = ft.OneHotEncoder(inputCols=['w_hand_class','l_hand_class','tourney_level_class'],outputCols=['w_hand_vec','l_hand_vec','tourney_level_vec'])\n",
    "# assembler\n",
    "assembler = ft.VectorAssembler(inputCols=[\"w_rank\",\"l_rank\",\"w_rank_bin\",\"l_rank_bin\",\n",
    "                                          \"minutes\",\"minutes_bin\",\n",
    "                                          \"w_height\",\"l_height\",\n",
    "                                          \"w_age\",\"l_age\",\n",
    "                                          \"w_hand_vec\",\"l_hand_vec\",\n",
    "                                         \"tourney_level_vec\"], outputCol=\"features\")\n",
    "# Principal Component Analysis\n",
    "######## Para elegir cuantos componentes elegir, he realizado el entrenamiento hasta pca, analizado de la siguiente manera variable a variable\n",
    "######## pca_model = pca.fit(test_predictions_df)\n",
    "######## pca_model.explainedVariance\n",
    "######## >> DenseVector([0.4912, 0.2797, 0.2166, 0.0052, 0.0047, 0.0014, 0.0011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "######## Aquí podemos obtener que pocentaje de conjunto es explicado por cada variable\n",
    "######## y he concluido que elegiría los 7 primeros componentes\n",
    "pca = ft.PCA(k=7, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "# Standard Scaler\n",
    "scaler = ft.StandardScaler(withMean=True, withStd=True, inputCol=\"pcaFeatures\",outputCol=\"std_features\")\n",
    "\n",
    "# Pipeline of estimators and transformers\n",
    "pipeline = Pipeline(stages=[binarizer_rank, binarizer_min, indexer, onehotencoder, assembler, pca, scaler])\n",
    "fitted_model = pipeline.fit(train_df)\n",
    "test_predictions_df = fitted_model.transform(train_df)\n",
    "\n",
    "test_predictions_df = test_predictions_df.drop('w_hand','l_hand',\n",
    "                                               'tourney_level',\n",
    "                                               'w_hand_class','l_hand_class',\n",
    "                                               'tourney_level_class',\n",
    "                                               'w_height','l_height',\n",
    "                                               'w_age','l_age',\n",
    "                                               'w_rank','l_rank',\n",
    "                                               'minutes','minutes_bin',\n",
    "                                               'w_rank_bin','l_rank_bin',\n",
    "                                               'w_hand_vec','l_hand_vec',\n",
    "                                               'tourney_level_vec',\n",
    "                                               'features',\n",
    "                                               'pcaFeatures')\n",
    "\n",
    "test_predictions_df.show(5, truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "#random_forest = RandomForestClassifier(featuresCol=\"std_features\",labelCol=\"class\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Regresión Logística (Multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión logística multinomial generaliza el metodo de regresión logística para problemas con multiples clases, es decir, se utiliza para predecir las probabilidades de los diferentes resultados posibles de una distribución categórica como variable dependiente, dado un conjunto de variables independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto hacen los arboles aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "random_forest = RandomForestClassifier(featuresCol=\"std_features\", labelCol=\"surface_class\",predictionCol=\"prediction\")\n",
    "\n",
    "# tuberia con el modelo de random forest\n",
    "rf_pipeline = Pipeline(stages=[binarizer_rank, binarizer_min, indexer, onehotencoder, assembler, pca, scaler, random_forest])\n",
    "\n",
    "# parametros a probar en la validación cruzada\n",
    "rf_parameters = (ParamGridBuilder()\n",
    "                .addGrid(random_forest.maxDepth, [4, 6, 8, 10, 12, 14])\n",
    "                .addGrid(random_forest.maxBins, [20, 22, 24, 25, 27])\n",
    "                .addGrid(random_forest.numTrees, [5, 6, 7, 8])\n",
    "                .build())\n",
    "\n",
    "# evaluador para probar la precisión del modelo\n",
    "rf_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"surface_class\", metricName=\"accuracy\")\n",
    "\n",
    "# configuración de la validación cruzada\n",
    "cross_validator = CrossValidator(estimator = rf_pipeline,\n",
    "                                estimatorParamMaps = rf_parameters,\n",
    "                                evaluator = rf_evaluator,\n",
    "                                numFolds = 5)\n",
    "\n",
    "# realizar la validación cruzada\n",
    "rf_model = cross_validator.fit(train_df)\n",
    "best_random_forest = rf_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a comprobar la precisión con el conjunto de test\n",
    "results = []\n",
    "best_results = []\n",
    "for (result, config) in zip(rf_model.avgMetrics, rf_parameters):\n",
    "    if result > 0.90:\n",
    "        best_results.append([result, config[random_forest.maxDepth], config[random_forest.maxBins], config[random_forest.numTrees]])\n",
    "    results.append([result, config[random_forest.maxDepth], config[random_forest.maxBins], config[random_forest.numTrees]])\n",
    "\n",
    "sorted(best_results, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a observar el modelo con mayor precisión\n",
    "random_forest_predictions = best_random_forest.transform(test_df)\n",
    "print(\"Random Forest: \", rf_evaluator.evaluate(random_forest_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Gradient-Boosted Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto hacen el Gradient-Boosted Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descripción de modelos seleccionados\n",
    "- Regresion logística:\n",
    "\n",
    "- Random Forest\n",
    "\n",
    "Random Forest es un metodo de aprendizaje para clasificación, regresión y otras tareas que se centran en construir multiples arboles de decisión a la hora de entrenar y sacando la clase que sea la moda de los arboles individuales.\n",
    "\n",
    "\n",
    "- Gradient-boosted tree"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Big Data",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
